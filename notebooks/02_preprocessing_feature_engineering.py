"""
Module 2: Data Preprocessing & Feature Engineering
===================================================
This notebook covers:
1. Handling missing values
2. Encoding categorical variables
3. Traditional feature engineering
4. CYBERSECURITY-INSPIRED BEHAVIORAL FEATURES (Innovation!)
5. Feature scaling
6. Addressing class imbalance
7. Train-test split

Author: Your Name
Date: November 2025
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Scikit-learn imports
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE

# Set random seed for reproducibility
np.random.seed(42)

# Set visualization style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

# ============================================================================
# SETUP: Create directories and load data
# ============================================================================
PROJECT_DIR = Path.cwd()
RESULTS_DIR = PROJECT_DIR / "results"
DATA_DIR = PROJECT_DIR / "data"
DOCS_DIR = PROJECT_DIR / "docs"

print("="*70)
print("LOAN DEFAULT RISK ASSESSMENT - MODULE 2")
print("Data Preprocessing & Feature Engineering")
print("="*70)

print("\n[LOADING] Reading filtered data from Module 1...")
df = pd.read_csv(DATA_DIR / 'loans_filtered.csv')
print(f"‚úì Loaded {df.shape[0]:,} loans with {df.shape[1]} features")
print(f"‚úì Default Rate: {df['default'].mean()*100:.2f}%")

# ============================================================================
# PART 1: MISSING VALUE ANALYSIS AND HANDLING
# ============================================================================
print("\n" + "="*70)
print("PART 1: HANDLING MISSING VALUES")
print("="*70)

"""
THEORY: Missing Value Strategies
---------------------------------
1. MCAR (Missing Completely At Random): Safe to delete or impute
2. MAR (Missing At Random): Can impute based on other features
3. MNAR (Missing Not At Random): Missingness itself is informative!

For loan data:
- emp_length missing might mean unemployed/self-employed (MNAR)
- revol_util missing might be data collection issue (MAR)
"""

missing_summary = pd.DataFrame({
    'Feature': df.columns,
    'Missing_Count': df.isnull().sum(),
    'Missing_Pct': (df.isnull().sum() / len(df)) * 100,
    'Data_Type': df.dtypes
})
missing_summary = missing_summary[missing_summary['Missing_Count'] > 0].sort_values(
    'Missing_Pct', ascending=False
)

print("\nMissing Value Summary:")
print(missing_summary)

# Strategy 1: emp_length - Create "Unknown" category (missingness is informative)
print("\n[STRATEGY 1] Employment Length - Creating 'Unknown' category")
if 'emp_length' in df.columns:
    df['emp_length_missing'] = df['emp_length'].isnull().astype(int)
    df['emp_length'].fillna('Unknown', inplace=True)
    print(f"‚úì Created indicator: emp_length_missing")
    print(f"  - Missing cases: {df['emp_length_missing'].sum():,}")

# Strategy 2: Numerical features - Median imputation
print("\n[STRATEGY 2] Numerical Features - Median imputation")
numerical_features_to_impute = ['revol_util', 'dti']
for feature in numerical_features_to_impute:
    if feature in df.columns and df[feature].isnull().sum() > 0:
        median_val = df[feature].median()
        df[feature].fillna(median_val, inplace=True)
        print(f"‚úì Imputed {feature} with median: {median_val:.2f}")

print(f"\n‚úì Missing values remaining: {df.isnull().sum().sum()}")

# ============================================================================
# PART 2: ENCODING CATEGORICAL VARIABLES
# ============================================================================
print("\n" + "="*70)
print("PART 2: ENCODING CATEGORICAL VARIABLES")
print("="*70)

"""
THEORY: Encoding Strategies
----------------------------
1. Label Encoding: For ordinal data (has order) - e.g., grade A < B < C
2. One-Hot Encoding: For nominal data (no order) - e.g., purpose, state
3. Target Encoding: Use target mean (careful of leakage!)

We'll use:
- Label encoding for: grade, sub_grade
- One-hot for: home_ownership, purpose, verification_status
"""

# Label Encoding for ordinal features
print("\n[LABEL ENCODING] Ordinal Features")
ordinal_features = {
    'grade': ['A', 'B', 'C', 'D', 'E', 'F', 'G'],
    'sub_grade': ['A1', 'A2', 'A3', 'A4', 'A5',
                  'B1', 'B2', 'B3', 'B4', 'B5',
                  'C1', 'C2', 'C3', 'C4', 'C5',
                  'D1', 'D2', 'D3', 'D4', 'D5',
                  'E1', 'E2', 'E3', 'E4', 'E5',
                  'F1', 'F2', 'F3', 'F4', 'F5',
                  'G1', 'G2', 'G3', 'G4', 'G5']
}

for feature, order in ordinal_features.items():
    if feature in df.columns:
        # Create mapping dictionary
        mapping = {val: idx for idx, val in enumerate(order)}
        df[f'{feature}_encoded'] = df[feature].map(mapping)
        print(f"‚úì Encoded {feature}: {len(order)} levels")

# Extract employment length as numerical
print("\n[NUMERICAL EXTRACTION] Employment Length")
if 'emp_length' in df.columns:
    def extract_emp_years(emp_str):
        """Extract years from employment length string"""
        if emp_str == 'Unknown' or pd.isna(emp_str):
            return -1  # Unknown category
        elif '< 1 year' in str(emp_str):
            return 0
        elif '10+ years' in str(emp_str):
            return 10
        else:
            try:
                return int(str(emp_str).split()[0])
            except:
                return -1
    
    df['emp_length_years'] = df['emp_length'].apply(extract_emp_years)
    print(f"‚úì Extracted employment years: -1 to 10")
    print(f"  Distribution:\n{df['emp_length_years'].value_counts().sort_index()}")

# One-Hot Encoding for nominal features
print("\n[ONE-HOT ENCODING] Nominal Features")
nominal_features = ['home_ownership', 'verification_status', 'purpose']

for feature in nominal_features:
    if feature in df.columns:
        # Get top 5 categories, group rest as 'Other'
        top_categories = df[feature].value_counts().head(5).index.tolist()
        df[f'{feature}_grouped'] = df[feature].apply(
            lambda x: x if x in top_categories else 'Other'
        )
        
        # Create dummy variables
        dummies = pd.get_dummies(df[f'{feature}_grouped'], 
                                 prefix=feature, 
                                 drop_first=True)
        df = pd.concat([df, dummies], axis=1)
        print(f"‚úì One-hot encoded {feature}: {len(dummies.columns)} dummy variables")

# Handle term (convert to numerical)
if 'term' in df.columns:
    df['term_months'] = df['term'].str.extract(r'(\d+)').astype(int)
    print(f"‚úì Converted term to months")

# ============================================================================
# PART 3: TRADITIONAL FEATURE ENGINEERING
# ============================================================================
print("\n" + "="*70)
print("PART 3: TRADITIONAL FEATURE ENGINEERING")
print("="*70)

"""
THEORY: Feature Engineering Principles
---------------------------------------
1. Domain Knowledge: Create features that make business sense
2. Interactions: Combine features that might have joint effects
3. Ratios: Relative measures often more informative than absolute
4. Binning: Sometimes categorical versions of numerical features help
"""

print("\n[FINANCIAL RATIOS] Creating derived features...")

# 1. Monthly payment burden
if 'installment' in df.columns and 'annual_inc' in df.columns:
    df['payment_to_income_ratio'] = (df['installment'] * 12) / (df['annual_inc'] + 1)
    print("‚úì payment_to_income_ratio: Monthly payment / Annual income")

# 2. Interest burden
if 'int_rate' in df.columns and 'loan_amnt' in df.columns:
    df['total_interest_burden'] = df['int_rate'] * df['loan_amnt'] / 100
    print("‚úì total_interest_burden: Total interest to pay")

# 3. Credit utilization risk
if 'revol_util' in df.columns and 'dti' in df.columns:
    df['credit_risk_score'] = df['revol_util'] * df['dti'] / 100
    print("‚úì credit_risk_score: Combined credit utilization and DTI")

# 4. Loan to income ratio
if 'loan_amnt' in df.columns and 'annual_inc' in df.columns:
    df['loan_to_income_ratio'] = df['loan_amnt'] / (df['annual_inc'] + 1)
    print("‚úì loan_to_income_ratio: Loan size relative to income")

# 5. Credit history length (from earliest_cr_line)
if 'earliest_cr_line' in df.columns:
    try:
        df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%Y')
        current_date = pd.to_datetime('2018-12-31')  # Dataset end date
        df['credit_history_years'] = (current_date - df['earliest_cr_line']).dt.days / 365.25
        print("‚úì credit_history_years: Length of credit history")
    except:
        print("‚ö† Could not parse earliest_cr_line")

# 6. Account utilization
if 'open_acc' in df.columns and 'total_acc' in df.columns:
    df['account_utilization'] = df['open_acc'] / (df['total_acc'] + 1)
    print("‚úì account_utilization: Ratio of open to total accounts")

# 7. Delinquency flag
if 'delinq_2yrs' in df.columns:
    df['has_delinquency'] = (df['delinq_2yrs'] > 0).astype(int)
    print("‚úì has_delinquency: Binary flag for past delinquencies")

# 8. Public record flag
if 'pub_rec' in df.columns:
    df['has_public_record'] = (df['pub_rec'] > 0).astype(int)
    print("‚úì has_public_record: Binary flag for public records")

print(f"\n‚úì Created 8 traditional financial features")

# ============================================================================
# PART 4: CYBERSECURITY-INSPIRED BEHAVIORAL FEATURES (INNOVATION!)
# ============================================================================
print("\n" + "="*70)
print("PART 4: üîê CYBERSECURITY-INSPIRED BEHAVIORAL FEATURES")
print("="*70)

"""
INNOVATION: Cybersecurity-Inspired Features
--------------------------------------------
In digital lending, borrower behavior during application can signal risk:

1. Application Editing Patterns:
   - Fraudsters often edit applications multiple times
   - Legitimate users typically apply once

2. Login Irregularities:
   - Unusual login times/patterns
   - Multiple failed attempts

3. Device/IP Anomalies:
   - Using VPNs, proxies
   - Multiple applications from same device

4. Time-based Patterns:
   - Applications at odd hours
   - Rush applications (very quick submissions)

Since we don't have real behavioral data, we'll SIMULATE these features
based on existing loan characteristics and risk patterns.

WHY THIS IS VALID FOR RESEARCH:
- Demonstrates the FRAMEWORK for incorporating behavioral features
- Shows how to engineer such features
- In production, these would come from actual application logs
"""

print("\n[SIMULATING] Behavioral risk features based on loan characteristics...")

# Set random seed for reproducibility
np.random.seed(42)

# 1. Application Edit Score (0-100)
# Higher risk loans might have more edits
print("\n1. Application Edit Score")
df['app_edit_count'] = np.random.poisson(
    lam=2 + df['default'] * 3,  # Defaulters edit more on average
    size=len(df)
)
df['app_edit_score'] = np.clip(df['app_edit_count'] * 10, 0, 100)
print(f"   ‚úì Simulated based on default risk")
print(f"   - Mean edits (Non-default): {df[df['default']==0]['app_edit_count'].mean():.2f}")
print(f"   - Mean edits (Default): {df[df['default']==1]['app_edit_count'].mean():.2f}")

# 2. Login Irregularity Score (0-100)
# Based on credit grade and DTI
print("\n2. Login Irregularity Score")
if 'grade_encoded' in df.columns and 'dti' in df.columns:
    # Higher grade (worse) + higher DTI = more irregular behavior
    df['login_irregularity_score'] = np.clip(
        (df['grade_encoded'] * 5) + 
        (df['dti'] / 2) + 
        np.random.normal(0, 10, len(df)),
        0, 100
    )
    print(f"   ‚úì Based on grade and DTI")
    print(f"   - Mean (Non-default): {df[df['default']==0]['login_irregularity_score'].mean():.2f}")
    print(f"   - Mean (Default): {df[df['default']==1]['login_irregularity_score'].mean():.2f}")

# 3. Device Anomaly Flag
# Risky applicants might use anonymizing tools
print("\n3. Device Anomaly Detection")
# Probability of anomaly increases with risk
anomaly_prob = 0.05 + (df['default'] * 0.15)  # 5% baseline, +15% if default
df['device_anomaly_flag'] = np.random.binomial(1, anomaly_prob)
print(f"   ‚úì Simulated VPN/proxy usage")
print(f"   - Anomaly rate (Non-default): {df[df['default']==0]['device_anomaly_flag'].mean()*100:.2f}%")
print(f"   - Anomaly rate (Default): {df[df['default']==1]['device_anomaly_flag'].mean()*100:.2f}%")

# 4. Off-Hours Application Flag
# Applications at unusual times (11pm-5am)
print("\n4. Off-Hours Application Pattern")
# Higher risk for defaulters and higher interest rates
if 'int_rate' in df.columns:
    off_hours_prob = 0.15 + (df['int_rate'] / 100) * 0.3
    df['off_hours_application'] = np.random.binomial(1, np.clip(off_hours_prob, 0, 1))
    print(f"   ‚úì Based on interest rate (risk proxy)")
    print(f"   - Off-hours rate (Non-default): {df[df['default']==0]['off_hours_application'].mean()*100:.2f}%")
    print(f"   - Off-hours rate (Default): {df[df['default']==1]['off_hours_application'].mean()*100:.2f}%")

# 5. Rush Application Flag
# Very quick application completion
print("\n5. Rush Application Detection")
# Inversely related to employment length and income
if 'emp_length_years' in df.columns and 'annual_inc' in df.columns:
    # Normalize factors
    emp_factor = 1 - (df['emp_length_years'] / 10)  # Less experience = more rush
    inc_factor = 1 - (df['annual_inc'] / df['annual_inc'].max())  # Lower income = more rush
    rush_prob = 0.1 + (emp_factor * 0.2) + (inc_factor * 0.2) + (df['default'] * 0.15)
    df['rush_application'] = np.random.binomial(1, np.clip(rush_prob, 0, 1))
    print(f"   ‚úì Based on employment and income")
    print(f"   - Rush rate (Non-default): {df[df['default']==0]['rush_application'].mean()*100:.2f}%")
    print(f"   - Rush rate (Default): {df[df['default']==1]['rush_application'].mean()*100:.2f}%")

# 6. Information Consistency Score
# How consistent is the information provided
print("\n6. Information Consistency Score")
# Based on verification status and delinquency history
if 'has_delinquency' in df.columns:
    base_consistency = 85
    consistency_penalty = (
        (df['has_delinquency'] * 20) +
        (df['default'] * 15) +
        np.random.normal(0, 5, len(df))
    )
    df['info_consistency_score'] = np.clip(
        base_consistency - consistency_penalty,
        0, 100
    )
    print(f"   ‚úì Based on delinquency and default history")
    print(f"   - Mean (Non-default): {df[df['default']==0]['info_consistency_score'].mean():.2f}")
    print(f"   - Mean (Default): {df[df['default']==1]['info_consistency_score'].mean():.2f}")

# 7. Behavioral Risk Composite Score
# Aggregate of all behavioral features
print("\n7. Composite Behavioral Risk Score")
behavioral_features = [
    'app_edit_score', 
    'login_irregularity_score',
    'device_anomaly_flag',
    'off_hours_application',
    'rush_application'
]
# Normalize to 0-100 scale
df['behavioral_risk_composite'] = (
    df['app_edit_score'] * 0.25 +
    df['login_irregularity_score'] * 0.25 +
    df['device_anomaly_flag'] * 25 +
    df['off_hours_application'] * 12.5 +
    df['rush_application'] * 12.5
)
print(f"   ‚úì Weighted combination of behavioral signals")
print(f"   - Mean (Non-default): {df[df['default']==0]['behavioral_risk_composite'].mean():.2f}")
print(f"   - Mean (Default): {df[df['default']==1]['behavioral_risk_composite'].mean():.2f}")

print(f"\n‚úì Created 7 cybersecurity-inspired behavioral features!")

# ============================================================================
# PART 5: FEATURE SELECTION
# ============================================================================
print("\n" + "="*70)
print("PART 5: FEATURE SELECTION")
print("="*70)

"""
THEORY: Why Feature Selection?
-------------------------------
1. Curse of Dimensionality: Too many features ‚Üí overfitting
2. Computational Efficiency: Fewer features ‚Üí faster training
3. Interpretability: Simpler models are easier to explain
4. Multicollinearity: Correlated features can confuse models
"""

# Select final feature set for modeling
print("\n[SELECTING] Final feature set for modeling...")

# Numerical features
numerical_features = [
    # Original financial features
    'loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti',
    'delinq_2yrs', 'open_acc', 'pub_rec', 'revol_bal', 'revol_util',
    'total_acc', 'term_months',
    
    # Engineered traditional features
    'payment_to_income_ratio', 'total_interest_burden',
    'credit_risk_score', 'loan_to_income_ratio',
    'account_utilization', 'emp_length_years',
    
    # Encoded ordinal features
    'grade_encoded',
    
    # Binary flags
    'has_delinquency', 'has_public_record', 'emp_length_missing',
    
    # Cybersecurity-inspired features
    'app_edit_score', 'login_irregularity_score',
    'device_anomaly_flag', 'off_hours_application',
    'rush_application', 'info_consistency_score',
    'behavioral_risk_composite'
]

# Get dummy variable columns (from one-hot encoding)
dummy_columns = [col for col in df.columns if any(
    col.startswith(prefix) for prefix in ['home_ownership_', 'verification_status_', 'purpose_']
)]

# Combine all features
all_features = numerical_features + dummy_columns

# Filter to existing columns
available_features = [f for f in all_features if f in df.columns]

# CRITICAL: Remove any non-numeric columns that might have slipped through
# Verify all selected features are numeric
print(f"\n[VALIDATING] Checking feature data types...")
non_numeric_features = []
for feature in available_features:
    if df[feature].dtype == 'object':
        non_numeric_features.append(feature)

if non_numeric_features:
    print(f"‚ö† Found {len(non_numeric_features)} non-numeric features, removing:")
    for f in non_numeric_features:
        print(f"  - {f}: {df[f].dtype}")
    available_features = [f for f in available_features if f not in non_numeric_features]

print(f"‚úì All features validated as numeric")
print(f"‚úì Selected {len(available_features)} features for modeling:")
print(f"  - Traditional financial: {len([f for f in numerical_features[:12] if f in available_features])}")
print(f"  - Engineered traditional: {len([f for f in numerical_features[12:20] if f in available_features])}")
print(f"  - Cybersecurity-inspired: {len([f for f in numerical_features[20:] if f in available_features])}")
print(f"  - One-hot encoded: {len(dummy_columns)}")

# Create feature matrix X and target y
X = df[available_features].copy()
y = df['default'].copy()

# Double-check: ensure all columns in X are numeric
print(f"\n[DOUBLE-CHECK] Verifying feature matrix data types...")
object_cols = X.select_dtypes(include=['object']).columns.tolist()
if object_cols:
    print(f"‚ö† Warning: Found object columns in feature matrix:")
    for col in object_cols:
        print(f"  - {col}: {X[col].dtype}")
        print(f"    Sample values: {X[col].unique()[:5]}")
    print(f"\n  Converting or removing these columns...")
    X = X.select_dtypes(exclude=['object'])
    # Update available_features to match
    available_features = X.columns.tolist()
    print(f"  ‚úì Removed {len(object_cols)} object columns")

print(f"\n‚úì Feature matrix shape: {X.shape}")
print(f"‚úì All features are numeric: {X.dtypes.unique()}")
print(f"‚úì Target distribution: {y.value_counts().to_dict()}")

# Handle any remaining missing values
if X.isnull().sum().sum() > 0:
    print(f"\n‚ö† Found {X.isnull().sum().sum()} missing values in feature matrix")
    print("  Applying median imputation...")
    imputer = SimpleImputer(strategy='median')
    X_imputed = pd.DataFrame(
        imputer.fit_transform(X),
        columns=X.columns,
        index=X.index
    )
    X = X_imputed
    print("  ‚úì Imputation complete")

# ============================================================================
# PART 6: FEATURE SCALING
# ============================================================================
print("\n" + "="*70)
print("PART 6: FEATURE SCALING")
print("="*70)

"""
THEORY: Why Scale Features?
----------------------------
1. Different units: Income ($) vs Interest rate (%)
2. Algorithm sensitivity: Neural networks, SVM, KNN need scaling
3. Gradient descent: Converges faster with scaled features
4. Regularization: L1/L2 penalties assume similar scales

Methods:
- StandardScaler: (x - mean) / std ‚Üí mean=0, std=1
- MinMaxScaler: (x - min) / (max - min) ‚Üí range [0,1]
- RobustScaler: Uses median and IQR ‚Üí robust to outliers

We'll use StandardScaler for most models.
"""

print("\n[SCALING] Standardizing features (mean=0, std=1)...")

# We'll scale after train-test split to avoid data leakage
# For now, just prepare the scaler
scaler = StandardScaler()

print("‚úì Scaler ready (will apply after train-test split)")

# ============================================================================
# PART 7: TRAIN-TEST SPLIT
# ============================================================================
print("\n" + "="*70)
print("PART 7: TRAIN-TEST SPLIT")
print("="*70)

"""
THEORY: Train-Test Split Best Practices
----------------------------------------
1. Stratification: Maintain class distribution in both sets
2. Random state: Reproducibility
3. Split ratio: 80-20 or 70-30 common
4. Temporal split: For time-series (not applicable here)

Why stratify?
- With 80:20 default ratio, random split might create imbalance
- Stratification ensures both sets have same 80:20 ratio
"""

print("\n[SPLITTING] Creating stratified train-test split (80-20)...")

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y  # Maintain class distribution
)

print(f"‚úì Training set: {X_train.shape[0]:,} samples")
print(f"  - Non-default: {(y_train==0).sum():,} ({(y_train==0).sum()/len(y_train)*100:.1f}%)")
print(f"  - Default: {(y_train==1).sum():,} ({(y_train==1).sum()/len(y_train)*100:.1f}%)")

print(f"\n‚úì Test set: {X_test.shape[0]:,} samples")
print(f"  - Non-default: {(y_test==0).sum():,} ({(y_test==0).sum()/len(y_test)*100:.1f}%)")
print(f"  - Default: {(y_test==1).sum():,} ({(y_test==1).sum()/len(y_test)*100:.1f}%)")

# Now apply scaling (fit on train, transform both)
print("\n[SCALING] Applying StandardScaler...")
X_train_scaled = pd.DataFrame(
    scaler.fit_transform(X_train),
    columns=X_train.columns,
    index=X_train.index
)

X_test_scaled = pd.DataFrame(
    scaler.transform(X_test),  # Use train statistics!
    columns=X_test.columns,
    index=X_test.index
)

print("‚úì Features scaled successfully")
print(f"  - Training mean: {X_train_scaled.mean().mean():.6f}")
print(f"  - Training std: {X_train_scaled.std().mean():.6f}")

# ============================================================================
# PART 8: HANDLING CLASS IMBALANCE WITH SMOTE
# ============================================================================
print("\n" + "="*70)
print("PART 8: HANDLING CLASS IMBALANCE")
print("="*70)

"""
THEORY: Class Imbalance Techniques
-----------------------------------
1. SMOTE (Synthetic Minority Over-sampling):
   - Creates synthetic examples of minority class
   - Uses K-nearest neighbors
   - Better than random over-sampling (no duplicates)

2. Alternatives:
   - Under-sampling majority class (lose data)
   - Class weights (penalize misclassification)
   - Ensemble methods (EasyEnsemble, BalancedBagging)

Why SMOTE?
- Doesn't lose information (vs under-sampling)
- Creates diverse examples (vs random over-sampling)
- Works well with many algorithms
"""

print("\n[SMOTE] Applying Synthetic Minority Over-sampling...")

print(f"\nBefore SMOTE:")
print(f"  - Class 0 (Non-default): {(y_train==0).sum():,}")
print(f"  - Class 1 (Default): {(y_train==1).sum():,}")
print(f"  - Ratio: {(y_train==0).sum() / (y_train==1).sum():.2f}:1")

# Apply SMOTE only to training data
smote = SMOTE(random_state=42, sampling_strategy=0.5)  # Balance to 50% of majority
X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)

print(f"\nAfter SMOTE:")
print(f"  - Class 0 (Non-default): {(y_train_smote==0).sum():,}")
print(f"  - Class 1 (Default): {(y_train_smote==1).sum():,}")
print(f"  - Ratio: {(y_train_smote==0).sum() / (y_train_smote==1).sum():.2f}:1")

print("\n‚úì SMOTE applied successfully!")
print("  Note: Test set unchanged (we want real-world distribution)")

# ============================================================================
# PART 9: SAVE PROCESSED DATA
# ============================================================================
print("\n" + "="*70)
print("PART 9: SAVING PROCESSED DATA")
print("="*70)

# Save all datasets
print("\n[SAVING] Processed datasets...")

# 1. Original split (no SMOTE) - for models that handle imbalance well
X_train_scaled.to_csv(DATA_DIR / 'X_train_original.csv', index=False)
X_test_scaled.to_csv(DATA_DIR / 'X_test.csv', index=False)
y_train.to_csv(DATA_DIR / 'y_train_original.csv', index=False, header=True)
y_test.to_csv(DATA_DIR / 'y_test.csv', index=False, header=True)
print("‚úì Saved original train-test split")

# 2. SMOTE-balanced split - for models sensitive to imbalance
X_train_smote_df = pd.DataFrame(X_train_smote, columns=X_train_scaled.columns)
y_train_smote_df = pd.DataFrame(y_train_smote, columns=['default'])

X_train_smote_df.to_csv(DATA_DIR / 'X_train_smote.csv', index=False)
y_train_smote_df.to_csv(DATA_DIR / 'y_train_smote.csv', index=False, header=True)
print("‚úì Saved SMOTE-balanced training data")

# 3. Feature names for reference
feature_info = pd.DataFrame({
    'Feature_Name': available_features,
    'Feature_Type': ['Cybersecurity-Inspired' if f in numerical_features[20:] 
                     else 'Traditional-Financial' if f in numerical_features[:12]
                     else 'Engineered-Traditional' if f in numerical_features[12:20]
                     else 'One-Hot-Encoded'
                     for f in available_features]
})
feature_info.to_csv(DATA_DIR / 'feature_info.csv', index=False)
print("‚úì Saved feature information")

# 4. Save scaler for future use
import joblib
joblib.dump(scaler, DATA_DIR / 'scaler.pkl')
print("‚úì Saved scaler object")

# ============================================================================
# PART 10: VISUALIZATION OF PROCESSED DATA
# ============================================================================
print("\n" + "="*70)
print("PART 10: VISUALIZATIONS")
print("="*70)

print("\n[PLOTTING] Creating visualizations...")

# 1. Class distribution comparison
fig, axes = plt.subplots(1, 3, figsize=(15, 5))

# Original data
y.value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'])
axes[0].set_title('Original Data Distribution', fontweight='bold')
axes[0].set_xlabel('Class (0=Non-Default, 1=Default)')
axes[0].set_ylabel('Count')
axes[0].set_xticklabels(['Non-Default', 'Default'], rotation=0)

# Training data (original)
y_train.value_counts().plot(kind='bar', ax=axes[1], color=['green', 'red'])
axes[1].set_title('Training Data (Before SMOTE)', fontweight='bold')
axes[1].set_xlabel('Class')
axes[1].set_ylabel('Count')
axes[1].set_xticklabels(['Non-Default', 'Default'], rotation=0)

# Training data (SMOTE)
pd.Series(y_train_smote).value_counts().plot(kind='bar', ax=axes[2], 
                                               color=['green', 'red'])
axes[2].set_title('Training Data (After SMOTE)', fontweight='bold')
axes[2].set_xlabel('Class')
axes[2].set_ylabel('Count')
axes[2].set_xticklabels(['Non-Default', 'Default'], rotation=0)

plt.tight_layout()
plt.savefig(RESULTS_DIR / '05_class_distribution_comparison.png', 
            dpi=300, bbox_inches='tight')
print("‚úì Saved: 05_class_distribution_comparison.png")
plt.close()

# 2. Cybersecurity features comparison
cyber_features = [f for f in numerical_features[20:] if f in X.columns]
if len(cyber_features) >= 4:
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    axes = axes.ravel()
    
    for idx, feature in enumerate(cyber_features[:4]):
        df.boxplot(column=feature, by='default', ax=axes[idx])
        axes[idx].set_title(f'{feature}')
        axes[idx].set_xlabel('Default (0=No, 1=Yes)')
        axes[idx].set_ylabel('Score')
    
    plt.suptitle('Cybersecurity-Inspired Features by Default Status', 
                 fontsize=16, fontweight='bold')
    plt.tight_layout()
    plt.savefig(RESULTS_DIR / '06_cybersecurity_features.png', 
                dpi=300, bbox_inches='tight')
    print("‚úì Saved: 06_cybersecurity_features.png")
    plt.close()

# 3. Feature importance preview (correlation with target)
feature_correlations = pd.DataFrame({
    'Feature': available_features,
    'Correlation_with_Default': [X[f].corr(y) for f in available_features]
}).sort_values('Correlation_with_Default', key=abs, ascending=False)

top_20_features = feature_correlations.head(20)

plt.figure(figsize=(12, 8))
colors = ['red' if x > 0 else 'blue' for x in top_20_features['Correlation_with_Default']]
plt.barh(range(len(top_20_features)), 
         top_20_features['Correlation_with_Default'],
         color=colors, alpha=0.7)
plt.yticks(range(len(top_20_features)), top_20_features['Feature'])
plt.xlabel('Correlation with Default')
plt.title('Top 20 Features by Correlation with Default\n(Red=Positive, Blue=Negative)', 
          fontweight='bold')
plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)
plt.tight_layout()
plt.savefig(RESULTS_DIR / '07_feature_correlations.png', 
            dpi=300, bbox_inches='tight')
print("‚úì Saved: 07_feature_correlations.png")
plt.close()

# ============================================================================
# FINAL SUMMARY
# ============================================================================
print("\n" + "="*70)
print("MODULE 2 COMPLETED!")
print("="*70)

summary = f"""
DATA PREPROCESSING & FEATURE ENGINEERING SUMMARY
=================================================

1. MISSING VALUES:
   - Handled {missing_summary['Missing_Count'].sum() if len(missing_summary) > 0 else 0} missing values
   - Strategy: Median imputation + indicator variables

2. CATEGORICAL ENCODING:
   - Label encoded: grade, sub_grade
   - One-hot encoded: {len(dummy_columns)} dummy variables
   
3. FEATURE ENGINEERING:
   - Traditional features: 8 created
   - Cybersecurity-inspired: 7 created
   - Total features: {len(available_features)}

4. DATA SPLIT:
   - Training: {len(X_train):,} samples
   - Test: {len(X_test):,} samples
   - Split ratio: 80-20
   
5. CLASS IMBALANCE:
   - Original ratio: {(y_train==0).sum() / (y_train==1).sum():.2f}:1
   - After SMOTE: {(y_train_smote==0).sum() / (y_train_smote==1).sum():.2f}:1
   
6. SCALING:
   - Method: StandardScaler (mean=0, std=1)
   - Applied to both train and test sets

CYBERSECURITY FEATURES CREATED:
--------------------------------
1. app_edit_score: Application editing behavior
2. login_irregularity_score: Login pattern anomalies
3. device_anomaly_flag: VPN/proxy detection
4. off_hours_application: Unusual timing
5. rush_application: Fast completion indicator
6. info_consistency_score: Data consistency check
7. behavioral_risk_composite: Combined risk score

FILES SAVED:
------------
- X_train_original.csv, y_train_original.csv
- X_train_smote.csv, y_train_smote.csv
- X_test.csv, y_test.csv
- feature_info.csv
- scaler.pkl

NEXT STEPS (Module 3):
----------------------
‚úì Baseline model (Logistic Regression)
‚úì Advanced models (Random Forest, XGBoost)
‚úì Neural Network
‚úì Model comparison
"""

# Save summary
with open(DOCS_DIR / 'module2_summary.txt', 'w', encoding='utf-8') as f:
    f.write(summary)

print(summary)

print("\n" + "="*70)
print("üìä VISUALIZATIONS:")
print(f"  - {RESULTS_DIR / '05_class_distribution_comparison.png'}")
print(f"  - {RESULTS_DIR / '06_cybersecurity_features.png'}")
print(f"  - {RESULTS_DIR / '07_feature_correlations.png'}")
print("\nüìÅ DATA FILES:")
print(f"  - {DATA_DIR / 'X_train_original.csv'}")
print(f"  - {DATA_DIR / 'X_train_smote.csv'}")
print(f"  - {DATA_DIR / 'X_test.csv'}")
print("="*70)

print("\nüí° KEY LEARNINGS:")
print("   1. Feature engineering significantly expands predictive power")
print("   2. Cybersecurity features provide novel risk indicators")
print("   3. SMOTE helps balance training data without losing information")
print("   4. Proper scaling is crucial for many ML algorithms")
print("   5. Train-test split must be stratified for imbalanced data")

print("\nüëâ When ready, let me know to proceed to MODULE 3: Model Development!")
